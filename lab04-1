In [1]:
import tensorflow as tf
import numpy as np

tf.enable_eager_execution()
tf.__version__
Out[1]:'1.12.0'

In [2]:
tf.set_random_seed(0)  # for reproducibility
In [3]:
x1_data = [1, 0, 3, 0, 5]
x2_data = [0, 2, 0, 4, 0]
y_data  = [1, 2, 3, 4, 5]

W1 = tf.Variable(tf.random_uniform([1], -10.0, 10.0))
W2 = tf.Variable(tf.random_uniform([1], -10.0, 10.0))
b  = tf.Variable(tf.random_uniform([1], -10.0, 10.0))

learning_rate = tf.Variable(0.001)

for i in range(1000+1):
    with tf.GradientTape() as tape:
        hypothesis = W1 * x1_data + W2 * x2_data + b
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))
    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])
    W1.assign_sub(learning_rate * W1_grad)
    W2.assign_sub(learning_rate * W2_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 50 == 0:
        print("{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}".format(
          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))
    0 | 335.280823 |    -4.0663 |     1.1220 |  -6.065215
   50 |  76.037262 |    -0.8001 |     1.6209 |  -4.978779
  100 |  18.959265 |     0.7151 |     1.8781 |  -4.429109
  150 |   6.310240 |     1.4125 |     2.0104 |  -4.134423
  200 |   3.445082 |     1.7284 |     2.0768 |  -3.961648
  250 |   2.743659 |     1.8667 |     2.1075 |  -3.847750
  300 |   2.525401 |     1.9225 |     2.1184 |  -3.762738
  350 |   2.417754 |     1.9402 |     2.1181 |  -3.692262
  400 |   2.337300 |     1.9403 |     2.1114 |  -3.629400
  450 |   2.264998 |     1.9325 |     2.1008 |  -3.570778
  500 |   2.196329 |     1.9213 |     2.0881 |  -3.514729
  550 |   2.130126 |     1.9085 |     2.0741 |  -3.460409
  600 |   2.066037 |     1.8953 |     2.0595 |  -3.407385
  650 |   2.003917 |     1.8819 |     2.0444 |  -3.355424
  700 |   1.943679 |     1.8686 |     2.0293 |  -3.304398
  750 |   1.885258 |     1.8555 |     2.0141 |  -3.254230
  800 |   1.828595 |     1.8425 |     1.9990 |  -3.204873
  850 |   1.773636 |     1.8297 |     1.9841 |  -3.156293
  900 |   1.720330 |     1.8171 |     1.9693 |  -3.108468
  950 |   1.668625 |     1.8048 |     1.9547 |  -3.061379
 1000 |   1.618474 |     1.7926 |     1.9403 |  -3.015011
 
 In [4]:
x_data = [
    [1., 0., 3., 0., 5.],
    [0., 2., 0., 4., 0.]
]
y_data  = [1, 2, 3, 4, 5]

W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))
b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))

learning_rate = tf.Variable(0.001)

for i in range(1000+1):
    with tf.GradientTape() as tape:
        hypothesis = tf.matmul(W, x_data) + b # (1, 2) * (2, 5) = (1, 5)
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))

        W_grad, b_grad = tape.gradient(cost, [W, b])
        W.assign_sub(learning_rate * W_grad)
        b.assign_sub(learning_rate * b_grad)
    
    if i % 50 == 0:
        print("{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}".format(
            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))
    0 |  36.403778 |    -0.6231 |    -0.3508 |  -0.961774
   50 |   9.372900 |     0.2914 |     0.1682 |  -0.557764
  100 |   2.639858 |     0.7060 |     0.4867 |  -0.347756
  150 |   0.825069 |     0.8912 |     0.6846 |  -0.235665
  200 |   0.284990 |     0.9721 |     0.8088 |  -0.174012
  250 |   0.106844 |     1.0062 |     0.8873 |  -0.138953
  300 |   0.042677 |     1.0195 |     0.9372 |  -0.118279
  350 |   0.018044 |     1.0241 |     0.9690 |  -0.105598
  400 |   0.008188 |     1.0250 |     0.9893 |  -0.097477
  450 |   0.004138 |     1.0246 |     1.0022 |  -0.092026
  500 |   0.002439 |     1.0239 |     1.0104 |  -0.088173
  550 |   0.001710 |     1.0230 |     1.0156 |  -0.085299
  600 |   0.001384 |     1.0223 |     1.0188 |  -0.083036
  650 |   0.001227 |     1.0217 |     1.0207 |  -0.081161
  700 |   0.001142 |     1.0212 |     1.0218 |  -0.079538
  750 |   0.001088 |     1.0207 |     1.0224 |  -0.078080
  800 |   0.001046 |     1.0203 |     1.0227 |  -0.076735
  850 |   0.001011 |     1.0199 |     1.0227 |  -0.075468
  900 |   0.000980 |     1.0196 |     1.0226 |  -0.074258
  950 |   0.000949 |     1.0192 |     1.0225 |  -0.073089
 1000 |   0.000921 |     1.0189 |     1.0222 |  -0.071954
 
 In [5]:
import tensorflow as tf

# 앞의 코드에서 bias(b)를 행렬에 추가
x_data = [
    [1., 1., 1., 1., 1.], # bias(b)
    [1., 0., 3., 0., 5.], 
    [0., 2., 0., 4., 0.]
]
y_data  = [1, 2, 3, 4, 5]

W = tf.Variable(tf.random_uniform([1, 3], -1.0, 1.0)) # [1, 3]으로 변경하고, b 삭제

learning_rate = 0.001
optimizer = tf.train.GradientDescentOptimizer(learning_rate)

for i in range(1000+1):
    with tf.GradientTape() as tape:
        hypothesis = tf.matmul(W, x_data) # b가 없다
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))

    grads = tape.gradient(cost, [W])
    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))
    if i % 50 == 0:
        print("{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}".format(
            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))
    0 |  16.019751 |    -0.1985 |     0.3424 |    -0.6835
   50 |   5.635924 |     0.0582 |     0.6809 |    -0.1215
  100 |   2.141112 |     0.1997 |     0.8238 |     0.2356
  150 |   0.862825 |     0.2786 |     0.8808 |     0.4641
  200 |   0.367090 |     0.3227 |     0.9015 |     0.6112
  250 |   0.167513 |     0.3468 |     0.9074 |     0.7064
  300 |   0.085210 |     0.3593 |     0.9082 |     0.7684
  350 |   0.050615 |     0.3649 |     0.9074 |     0.8090
  400 |   0.035731 |     0.3663 |     0.9067 |     0.8359
  450 |   0.029064 |     0.3651 |     0.9063 |     0.8539
  500 |   0.025846 |     0.3624 |     0.9064 |     0.8661
  550 |   0.024085 |     0.3587 |     0.9069 |     0.8746
  600 |   0.022948 |     0.3544 |     0.9076 |     0.8807
  650 |   0.022085 |     0.3497 |     0.9086 |     0.8852
  700 |   0.021348 |     0.3449 |     0.9097 |     0.8887
  750 |   0.020676 |     0.3400 |     0.9109 |     0.8916
  800 |   0.020042 |     0.3350 |     0.9121 |     0.8940
  850 |   0.019434 |     0.3301 |     0.9133 |     0.8960
  900 |   0.018848 |     0.3252 |     0.9146 |     0.8979
  950 |   0.018280 |     0.3203 |     0.9158 |     0.8997
 1000 |   0.017730 |     0.3155 |     0.9171 |     0.9013
 
 In [6]:
# Multi-variable linear regression (1)

X = tf.constant([[1., 2.], 
                 [3., 4.]])
y = tf.constant([[1.5], [3.5]])

W = tf.Variable(tf.random_normal([2, 1]))
b = tf.Variable(tf.random_normal([1]))

# Create an optimizer
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)

n_epoch = 1000+1
print("epoch | cost")
for i in range(n_epoch):
    # Use tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        y_pred = tf.matmul(X, W) + b
        cost = tf.reduce_mean(tf.square(y_pred - y))

    # calculates the gradients of the loss
    grads = tape.gradient(cost, [W, b])
    
    # updates parameters (W and b)
    optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))
    if i % 50 == 0:
        print("{:5} | {:10.6f}".format(i, cost.numpy()))
epoch | cost
    0 |  21.726822
   50 |   0.258689
  100 |   0.176868
  150 |   0.120926
  200 |   0.082678
  250 |   0.056528
  300 |   0.038649
  350 |   0.026424
  400 |   0.018067
  450 |   0.012352
  500 |   0.008445
  550 |   0.005774
  600 |   0.003948
  650 |   0.002699
  700 |   0.001845
  750 |   0.001262
  800 |   0.000863
  850 |   0.000590
  900 |   0.000403
  950 |   0.000276
 1000 |   0.000189
 
 In [7]:
tf.set_random_seed(0)  # for reproducibility
x1 = [ 73.,  93.,  89.,  96.,  73.]
x2 = [ 80.,  88.,  91.,  98.,  66.]
x3 = [ 75.,  93.,  90., 100.,  70.]
Y  = [152., 185., 180., 196., 142.]

# weights
w1 = tf.Variable(10.)
w2 = tf.Variable(10.)
w3 = tf.Variable(10.)
b  = tf.Variable(10.)

hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b
In [8]:
# data and label
x1 = [ 73.,  93.,  89.,  96.,  73.]
x2 = [ 80.,  88.,  91.,  98.,  66.]
x3 = [ 75.,  93.,  90., 100.,  70.]
Y  = [152., 185., 180., 196., 142.]

# weights
w1 = tf.Variable(10.)
w2 = tf.Variable(10.)
w3 = tf.Variable(10.)
b  = tf.Variable(10.)

learning_rate = 0.000001

for i in range(1000+1):
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b
        cost = tf.reduce_mean(tf.square(hypothesis - Y))
    # calculates the gradients of the cost
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])
    
    # update w1,w2,w3 and b
    w1.assign_sub(learning_rate * w1_grad)
    w2.assign_sub(learning_rate * w2_grad)
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 50 == 0:
      print("{:5} | {:12.4f}".format(i, cost.numpy()))
    0 | 5793889.5000
   50 |   64291.1484
  100 |     715.2902
  150 |       9.8462
  200 |       2.0152
  250 |       1.9252
  300 |       1.9210
  350 |       1.9177
  400 |       1.9145
  450 |       1.9114
  500 |       1.9081
  550 |       1.9050
  600 |       1.9018
  650 |       1.8986
  700 |       1.8955
  750 |       1.8923
  800 |       1.8892
  850 |       1.8861
  900 |       1.8829
  950 |       1.8798
 1000 |       1.8767
 
 In [9]:
# data and label
x1 = [ 73.,  93.,  89.,  96.,  73.]
x2 = [ 80.,  88.,  91.,  98.,  66.]
x3 = [ 75.,  93.,  90., 100.,  70.]
Y  = [152., 185., 180., 196., 142.]

# random weights
w1 = tf.Variable(tf.random_normal([1]))
w2 = tf.Variable(tf.random_normal([1]))
w3 = tf.Variable(tf.random_normal([1]))
b  = tf.Variable(tf.random_normal([1]))

learning_rate = 0.000001

for i in range(1000+1):
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b
        cost = tf.reduce_mean(tf.square(hypothesis - Y))
    # calculates the gradients of the cost
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])
    
    # update w1,w2,w3 and b
    w1.assign_sub(learning_rate * w1_grad)
    w2.assign_sub(learning_rate * w2_grad)
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 50 == 0:
      print("{:5} | {:12.4f}".format(i, cost.numpy()))
    0 |   11325.9121
   50 |     135.3618
  100 |      11.1817
  150 |       9.7940
  200 |       9.7687
  250 |       9.7587
  300 |       9.7489
  350 |       9.7389
  400 |       9.7292
  450 |       9.7194
  500 |       9.7096
  550 |       9.6999
  600 |       9.6903
  650 |       9.6806
  700 |       9.6709
  750 |       9.6612
  800 |       9.6517
  850 |       9.6421
  900 |       9.6325
  950 |       9.6229
 1000 |       9.6134
 
 In [10]:
data = np.array([
    # X1,   X2,    X3,   y
    [ 73.,  80.,  75., 152. ],
    [ 93.,  88.,  93., 185. ],
    [ 89.,  91.,  90., 180. ],
    [ 96.,  98., 100., 196. ],
    [ 73.,  66.,  70., 142. ]
], dtype=np.float32)

# slice data
X = data[:, :-1]
y = data[:, [-1]]

W = tf.Variable(tf.random_normal([3, 1]))
b = tf.Variable(tf.random_normal([1]))

learning_rate = 0.000001

# hypothesis, prediction function
def predict(X):
    return tf.matmul(X, W) + b

print("epoch | cost")

n_epochs = 2000
for i in range(n_epochs+1):
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        cost = tf.reduce_mean((tf.square(predict(X) - y)))

    # calculates the gradients of the loss
    W_grad, b_grad = tape.gradient(cost, [W, b])

    # updates parameters (W and b)
    W.assign_sub(learning_rate * W_grad)
    b.assign_sub(learning_rate * b_grad)
    
    if i % 100 == 0:
        print("{:5} | {:10.4f}".format(i, cost.numpy()))
epoch | cost
    0 |  5455.5903
  100 |    31.7443
  200 |    30.9326
  300 |    30.7894
  400 |    30.6468
  500 |    30.5055
  600 |    30.3644
  700 |    30.2242
  800 |    30.0849
  900 |    29.9463
 1000 |    29.8081
 1100 |    29.6710
 1200 |    29.5348
 1300 |    29.3989
 1400 |    29.2641
 1500 |    29.1299
 1600 |    28.9961
 1700 |    28.8634
 1800 |    28.7313
 1900 |    28.5997
 2000 |    28.4689
In [11]:
W.numpy()
Out[11]:
array([[ 1.368576 ],
       [ 2.1047728],
       [-1.4229954]], dtype=float32)
In [12]:
b.numpy()
Out[12]:
array([-1.1783521], dtype=float32)
In [13]:
tf.matmul(X, W) + b
Out[13]:
<tf.Tensor: id=383121, shape=(5, 1), dtype=float32, numpy=
array([[160.38487],
       [178.98064],
       [184.08965],
       [194.17314],
       [138.03304]], dtype=float32)>
       
       In [14]:
Y # labels, 실제값
Out[14]:
[152.0, 185.0, 180.0, 196.0, 142.0]
In [15]:
predict(X).numpy() # prediction, 예측값
Out[15]:
array([[160.38487],
       [178.98064],
       [184.08965],
       [194.17314],
       [138.03304]], dtype=float32)
In [16]:
# 새로운 데이터에 대한 예측

predict([[ 89.,  95.,  92.],[ 84.,  92.,  85.]]).numpy()
Out[16]:
array([[189.66275],
       [186.46652]], dtype=float32)
